{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mksachs/CCVC/blob/main/notebooks/Unit%204%20-%20Multimodal%20Models/Clip_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4ba8f5-ab09-434c-8165-15d766c2654e",
      "metadata": {
        "id": "bf4ba8f5-ab09-434c-8165-15d766c2654e"
      },
      "source": [
        "# CLIP transfer learning\n",
        "---\n",
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9206554a-f418-437f-9ca8-3b7a5852b0c5",
      "metadata": {
        "id": "9206554a-f418-437f-9ca8-3b7a5852b0c5"
      },
      "source": [
        "In this notebook we will look at how to fine-tune CLIP models for the [Oxford-pets](https://huggingface.co/datasets/pcuenq/oxford-pets) dataset.\n",
        "\n",
        "[CLIP](https://arxiv.org/abs/2103.00020), or Contrastive Language-Image Pre-training, is a multimodal model that combines language and vision to extract features from text and images. The resulting features are then projected into a common Euclidean space, allowing for meaningful comparisons between text and image features using the dot product.\n",
        "\n",
        "Normally, CLIP operates in zero-shot prediction mode or functions as a feature extractor. Consequently, fine-tuning CLIP diverges from the usual process, as seen in tasks like fine-tuning ResNet for classification. In our scenario, the model must acquire the most valuable features from both the input text and image data.\n",
        "\n",
        "Evaluating the quality of the features extracted remains an ongoing area of active research. One method employed for this assessment involves utilizing the extracted features to address the classification problem, either through the application of a [linear classifier probe](https://h2o.ai/wiki/probing-classifiers/) or adopting a zero-shot approach.\n",
        "\n",
        "---\n",
        "\n",
        "Let's start with importing necessary functions, libraries and environmental variables."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets evaluate transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxahSLNuRLbo",
        "outputId": "f5bdfa63-7f7a-4235-fa30-c904c6a93395"
      },
      "id": "nxahSLNuRLbo",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m481.3/491.2 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "49753ed9-8db5-46a5-81fe-c72fb6448bcc",
      "metadata": {
        "id": "49753ed9-8db5-46a5-81fe-c72fb6448bcc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from functools import partial\n",
        "from typing import Any\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import (CLIPImageProcessor, CLIPModel, CLIPProcessor,\n",
        "                          CLIPTokenizerFast, Trainer, TrainingArguments)\n",
        "from transformers.tokenization_utils_base import BatchEncoding\n",
        "from datasets.formatting.formatting import LazyBatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5fddf114-a640-44c8-a9c1-b11341c8b3fd",
      "metadata": {
        "id": "5fddf114-a640-44c8-a9c1-b11341c8b3fd"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CURL_CA_BUNDLE\"] = \"\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def seed_all(seed: int):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "seed_all(69)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d8eee91-69ac-44f1-b867-09da69a64a86",
      "metadata": {
        "id": "9d8eee91-69ac-44f1-b867-09da69a64a86"
      },
      "source": [
        "### Exploratory Data Analysis on the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c424484-bb31-4848-9409-8f24cacb289e",
      "metadata": {
        "id": "1c424484-bb31-4848-9409-8f24cacb289e"
      },
      "source": [
        "We'll utilize the [ğŸ¤—datasets](https://huggingface.co/docs/datasets/index) library to load the [Oxford-pets](https://huggingface.co/datasets/pcuenq/oxford-pets) dataset, comprising 37 pet categories (primarily dogs and cats) with a total of 7,390 images.\n",
        "\n",
        "Since the dataset exclusively consists of the training portion, we'll proceed to partition it into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3cd09d-2fc9-45d3-aa1e-1c80fc6681af",
      "metadata": {
        "id": "ce3cd09d-2fc9-45d3-aa1e-1c80fc6681af"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"pcuenq/oxford-pets\")\n",
        "dataset_train_val = dataset['train'].train_test_split(test_size=0.3)\n",
        "dataset_val_test = dataset_train_val['test'].train_test_split(test_size=0.2)\n",
        "\n",
        "dataset = DatasetDict(\n",
        "    {\n",
        "        \"train\": dataset_train_val['train'],\n",
        "        \"val\": dataset_val_test['test'],\n",
        "        \"test\": dataset_val_test['train']\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af81bbc1-f17f-450d-9ae0-f8a57680eb1c",
      "metadata": {
        "id": "af81bbc1-f17f-450d-9ae0-f8a57680eb1c"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52544e43-a02b-416f-94e5-3221f4d23a41",
      "metadata": {
        "id": "52544e43-a02b-416f-94e5-3221f4d23a41"
      },
      "source": [
        "An individual item in the dataset consists of the following attributes:\n",
        "\n",
        "- **path**: A string representing the image file path.\n",
        "- **label**: A string denoting the pet category.\n",
        "- **dog**: A boolean value; True if the image features a dog.\n",
        "- **image**: A PIL image representing the dataset image.\n",
        "\n",
        "However, for our purposes, we will exclusively utilize the **label** and **image** columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3c8fd1-d217-4ede-a3b0-64f627f30878",
      "metadata": {
        "id": "dd3c8fd1-d217-4ede-a3b0-64f627f30878",
        "outputId": "17715a60-ae6e-4d89-fcae-f65efca1ffb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'path': '/data/datasets/magic-ml/oxford-iiit-pet/images/saint_bernard_19.jpg',\n",
              " 'label': 'saint bernard',\n",
              " 'dog': True,\n",
              " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375>}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce7b298-d4c2-4193-bad2-6458fce390a1",
      "metadata": {
        "id": "6ce7b298-d4c2-4193-bad2-6458fce390a1"
      },
      "source": [
        "Now, let's examine a selection of images along with their corresponding labels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e86a2406-40fd-4cbb-9350-465d2b7f2291",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "e86a2406-40fd-4cbb-9350-465d2b7f2291",
        "outputId": "0004fb2b-a266-4009-a3f9-73edafadb149"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9ddf5131cb8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mshow_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "def show_samples(ds,rows,cols):\n",
        "    samples = ds.shuffle().select(np.arange(rows*cols)) # selecting random images\n",
        "    fig = plt.figure(figsize=(cols*4,rows*4))\n",
        "    # plotting\n",
        "    for i in range(rows*cols):\n",
        "        img_bytes = samples[i]['image']['bytes']\n",
        "        img = Image.open(io.BytesIO(img_bytes))\n",
        "        label = samples[i]['label']\n",
        "        fig.add_subplot(rows,cols,i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(label)\n",
        "        plt.axis('off')\n",
        "\n",
        "show_samples(dataset['train'],rows=3,cols=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c92d22e-bc9a-43ea-bed7-cd8c35b867c2",
      "metadata": {
        "id": "8c92d22e-bc9a-43ea-bed7-cd8c35b867c2"
      },
      "source": [
        "For fine-tuning we need process labels to ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71dbf018-8937-4700-9b38-3e4201d43059",
      "metadata": {
        "id": "71dbf018-8937-4700-9b38-3e4201d43059"
      },
      "outputs": [],
      "source": [
        "labels = set(dataset['train']['label'])\n",
        "label2id = {label: i for i, label in enumerate(labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "labels = list(label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3345137f-b183-4b38-9a2a-40335282d190",
      "metadata": {
        "id": "3345137f-b183-4b38-9a2a-40335282d190"
      },
      "source": [
        "After that we get mapping from label to id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d5a990-3871-4104-b29d-b1e2d395d24d",
      "metadata": {
        "id": "25d5a990-3871-4104-b29d-b1e2d395d24d",
        "outputId": "787ffe0e-757d-48be-fd63-bd686fa420d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'wheaten terrier': 0,\n",
              " 'beagle': 1,\n",
              " 'boxer': 2,\n",
              " 'pug': 3,\n",
              " 'Birman': 4,\n",
              " 'Bombay': 5,\n",
              " 'american bulldog': 6,\n",
              " 'staffordshire bull terrier': 7,\n",
              " 'english setter': 8,\n",
              " 'yorkshire terrier': 9,\n",
              " 'Bengal': 10,\n",
              " 'havanese': 11,\n",
              " 'shiba inu': 12,\n",
              " 'British Shorthair': 13,\n",
              " 'Egyptian Mau': 14,\n",
              " 'japanese chin': 15,\n",
              " 'samoyed': 16,\n",
              " 'Ragdoll': 17,\n",
              " 'basset hound': 18,\n",
              " 'Sphynx': 19,\n",
              " 'english cocker spaniel': 20,\n",
              " 'american pit bull terrier': 21,\n",
              " 'miniature pinscher': 22,\n",
              " 'Persian': 23,\n",
              " 'Maine Coon': 24,\n",
              " 'leonberger': 25,\n",
              " 'newfoundland': 26,\n",
              " 'Russian Blue': 27,\n",
              " 'saint bernard': 28,\n",
              " 'chihuahua': 29,\n",
              " 'Abyssinian': 30,\n",
              " 'scottish terrier': 31,\n",
              " 'keeshond': 32,\n",
              " 'pomeranian': 33,\n",
              " 'Siamese': 34,\n",
              " 'german shorthaired': 35,\n",
              " 'great pyrenees': 36}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285a9c46-76d4-4e69-983d-7c5d1da73389",
      "metadata": {
        "id": "285a9c46-76d4-4e69-983d-7c5d1da73389"
      },
      "source": [
        "### Dataset preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9feacfe-9612-4a0e-a1bb-ba8e96de7616",
      "metadata": {
        "id": "a9feacfe-9612-4a0e-a1bb-ba8e96de7616"
      },
      "source": [
        "For the default CLIP model, we will use the pre-trained [model by OpenAI](https://huggingface.co/openai/clip-vit-base-patch32). However, feel free to modify the `MODEL_NAME` variable to another model. For instance [this](https://huggingface.co/openai/clip-vit-large-patch14) model of CLIP have a larger number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2db29d-56f9-415a-95b7-0cc54a5cc7b9",
      "metadata": {
        "id": "6e2db29d-56f9-415a-95b7-0cc54a5cc7b9"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"openai/clip-vit-base-patch32\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d784b5c7-8d5a-41b8-9af0-b8962e83ec3f",
      "metadata": {
        "id": "d784b5c7-8d5a-41b8-9af0-b8962e83ec3f"
      },
      "source": [
        "For selected `MODEL_NAME` we need to import corresponding:\n",
        "- **Tokenizer**: used to encode input text.\n",
        "- **Image processor**: It is utilized to preprocess input images, handling tasks such as scaling, resizing, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407c107e-b7c4-4291-928b-eee5886cc84d",
      "metadata": {
        "id": "407c107e-b7c4-4291-928b-eee5886cc84d"
      },
      "outputs": [],
      "source": [
        "TOKENIZER = CLIPTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "IMAGE_PROCESSOR = CLIPImageProcessor.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4b8cfd-72ee-42ca-ac77-d343a363eb31",
      "metadata": {
        "id": "1c4b8cfd-72ee-42ca-ac77-d343a363eb31"
      },
      "source": [
        "Dataset preparation consists of 2 steps\n",
        "1) **Label Encoding with Tokenizer**: This step entails encoding the label of each dataset item using the tokenizer. Importantly, we encode not only the label's text but also include a special prompt with the label. This enhancement is designed to elevate the quality of text features in the language model.\n",
        "\n",
        "2) **Image Preparations with CLIP's Image Processor**: In this step, we prepare the images using CLIP's image processor. This involves preprocessing steps to ensure compatibility with CLIP's model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e1aa6c-346d-485d-9432-e0f2999f516d",
      "metadata": {
        "id": "e3e1aa6c-346d-485d-9432-e0f2999f516d"
      },
      "outputs": [],
      "source": [
        "def transform_class_labels(\n",
        "    items: LazyBatch, tokenizer: CLIPTokenizerFast, label2id: dict[str, int]\n",
        ") -> dict[str, Any]:\n",
        "    \"\"\"Function used to encode item's label prompt with tokenizer\n",
        "\n",
        "    Args:\n",
        "        items (LazyBatch): Input dataset items\n",
        "        tokenizer (CLIPTokenizerFast): CLIP's tokenizer\n",
        "        label2id (dict[str, int]): mapping from label to id\n",
        "\n",
        "    Returns:\n",
        "        dict[str, Any]: transformed items\n",
        "    \"\"\"\n",
        "    # this prompt is better than prompt consisting only of the class name\n",
        "    label_prompt = [f\"a photo of {label}\" for label in items[\"label\"]]\n",
        "    output = tokenizer(label_prompt, padding=True, return_tensors=\"pt\")\n",
        "    items[\"input_ids\"] = output[\"input_ids\"]\n",
        "    items[\"attention_mask\"] = output[\"attention_mask\"]\n",
        "    items[\"label_id\"] = [label2id[label] for label in items[\"label\"]]\n",
        "    return items\n",
        "\n",
        "\n",
        "def transform_image(items: LazyBatch, image_processor: CLIPImageProcessor) -> dict[str, Any]:\n",
        "    \"\"\"Function used to preprocess input image with image processor\n",
        "\n",
        "    Args:\n",
        "        items (LazyBatch): Input dataset items\n",
        "        image_processor (CLIPImageProcessor): CLIP's image processor\n",
        "\n",
        "    Returns:\n",
        "        dict[str, Any]: transformed items\n",
        "    \"\"\"\n",
        "    output = image_processor(items[\"image\"], return_tensors=\"pt\")\n",
        "    items[\"pixel_values\"] = output[\"pixel_values\"]\n",
        "    return items"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ec599d-3bf4-4e83-98f8-86fd950e94c3",
      "metadata": {
        "id": "23ec599d-3bf4-4e83-98f8-86fd950e94c3"
      },
      "source": [
        "We can efficiently apply `transform_class_labels` function on each dataset split with [map function](https://huggingface.co/docs/datasets/process#map) inplace.\n",
        "\n",
        "But for `transform_image` function we'll use [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform) to lazily process pictures because it's inefficient to store them in memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c0b7d07-cd17-4ba5-834a-71950a489a3d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "18639d88c97243699d15434f0afc6540",
            "569b04c736c6490890653c08c425e32a",
            "7ed84ef7cfdc436292e7c8c107105481"
          ]
        },
        "id": "5c0b7d07-cd17-4ba5-834a-71950a489a3d",
        "outputId": "0e7d2487-4add-425f-a533-ebc81a0353bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18639d88c97243699d15434f0afc6540",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5173 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "569b04c736c6490890653c08c425e32a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/444 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ed84ef7cfdc436292e7c8c107105481",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1773 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = dataset.map(partial(transform_class_labels, tokenizer=TOKENIZER, label2id=label2id), batched=True)\n",
        "dataset.set_transform(partial(transform_image, image_processor=IMAGE_PROCESSOR))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9431944-cfd8-4db8-83a4-d229a20bb49c",
      "metadata": {
        "id": "b9431944-cfd8-4db8-83a4-d229a20bb49c"
      },
      "source": [
        "### CLIP Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3762d8-d5d2-4fbd-893d-f77119fbae4b",
      "metadata": {
        "id": "cc3762d8-d5d2-4fbd-893d-f77119fbae4b"
      },
      "outputs": [],
      "source": [
        "# utilities functions\n",
        "def get_module_device(module: nn.Module) -> torch.device:\n",
        "    \"\"\"Function to get current module device\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): pytorch module\n",
        "\n",
        "    Returns:\n",
        "        torch.device: module's device\n",
        "    \"\"\"\n",
        "    return next(module.parameters()).device\n",
        "\n",
        "\n",
        "def freeze_params(module: nn.Module, freeze_top_percent: float = 1.0) -> None:\n",
        "    \"\"\"Function used to freeze module by setting 'requires_grad' to False\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): pytorch module\n",
        "        freeze_top_percent (float, optional): percentage of layers starting from the top that need to be frozen. Defaults to 1.0.\n",
        "    \"\"\"\n",
        "    all_params_length = len(list(module.parameters()))\n",
        "    for indx, param in enumerate(module.parameters()):\n",
        "        if int(all_params_length * freeze_top_percent) <= indx:\n",
        "            break\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model: nn.Module) -> None:\n",
        "    \"\"\"Function prints statistics about trainable parameters\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): pytorch module\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"Trainable params: {(trainable_params / 10**6):.4f}M || All params: {(all_param / 10**6):.4f}M || Trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5586412-47c9-4930-81dd-5c65747de762",
      "metadata": {
        "id": "f5586412-47c9-4930-81dd-5c65747de762"
      },
      "source": [
        "Our CLIP based classification model will work as follows:\n",
        "1. **Prompt Generation**: Initial prompts are generated based on the labels present in the dataset.\n",
        "2. **Feature Extraction with Language CLIP Model**: Features are extracted from these prompts using a language CLIP model.\n",
        "3. **Visual Feature Extraction**: For each image, features are extracted using a visual CLIP model.\n",
        "4. **Similarity Score Computation**: A similarity score between the image and the set of prompts is computed.\n",
        "5. **Prediction Based on Similarity**: The most similar prompt is selected, and its associated label is used as the final prediction for the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b1fbeb-f7b1-45b9-a0fe-aa27374c88fb",
      "metadata": {
        "id": "a4b1fbeb-f7b1-45b9-a0fe-aa27374c88fb"
      },
      "outputs": [],
      "source": [
        "class CLIPClassifier(nn.Module):\n",
        "    def __init__(self, clip_model: CLIPModel, tokenizer: CLIPTokenizerFast, labels: list[str]):\n",
        "        super().__init__()\n",
        "        self.model = clip_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.logit_scale = self.model.logit_scale.exp()\n",
        "        self.label2id = {label: i for i, label in enumerate(labels)}\n",
        "        self.labels_embeddings = nn.Parameter(self.generate_labels_embeddings(labels))\n",
        "\n",
        "    def generate_labels_embeddings(self, labels: list[str]) -> torch.Tensor:\n",
        "        \"\"\"Function prepares label's prompt embeddings usings language model\n",
        "\n",
        "        Args:\n",
        "            labels (list[str]): list with dataset labels\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: labels embeddings of size (num_labels, dim)\n",
        "        \"\"\"\n",
        "        # tokenize label's prompt\n",
        "        labels_inputs = self.tokenizer(\n",
        "            [f\"a photo of {label}\" for label in labels],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        ).to(get_module_device(self.model))\n",
        "        # run language model on this prompt and get embeddings\n",
        "        labels_embeddings = self.model.get_text_features(**labels_inputs)\n",
        "        # normilize embeddings\n",
        "        labels_embeddings /= labels_embeddings.norm(p=2, dim=-1, keepdim=True)\n",
        "        return labels_embeddings\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Function to calculate similarity between image and label's promts embeddings\n",
        "\n",
        "        Args:\n",
        "            images (torch.Tensor): image tensor\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: output similarity tensor of size (batch_size,  num_labels)\n",
        "        \"\"\"\n",
        "        image_features = self.model.get_image_features(\n",
        "            images\n",
        "        )  # (batch_size, dim)\n",
        "        image_features /= image_features.norm(p=2, dim=-1, keepdim=True)\n",
        "        return torch.matmul(image_features, self.labels_embeddings.T) * self.logit_scale\n",
        "\n",
        "def calculate_accuracy(model: CLIPClassifier, dataloader: DataLoader) -> float:\n",
        "    \"\"\"Function calculates accuracy of given model on dataloader\n",
        "\n",
        "    Args:\n",
        "        model (CLIPClassifier): CLIP classifier model\n",
        "        dataloader (DataLoader): evaluation dataloader\n",
        "\n",
        "    Returns:\n",
        "        float: model's accuracy\n",
        "    \"\"\"\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    predictions_list = []\n",
        "    references_list = []\n",
        "    device = get_module_device(model)\n",
        "    for batch in tqdm(\n",
        "        dataloader, total=len(dataloader), desc=\"Evaluate model on dataset\"\n",
        "    ):\n",
        "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
        "        predictions = model(batch[\"pixel_values\"])\n",
        "        predictions_list.append(torch.argmax(predictions, dim=1))\n",
        "        references_list.append(batch[\"label_id\"])\n",
        "    return metric.compute(\n",
        "        predictions=torch.concat(predictions_list),\n",
        "        references=torch.concat(references_list),\n",
        "    )[\"accuracy\"]\n",
        "\n",
        "def collate_fn(items: LazyBatch) -> dict[str, Any]:\n",
        "    return {\n",
        "        \"pixel_values\": torch.stack(\n",
        "            [item[\"pixel_values\"] for item in items]\n",
        "        ),  # (batch_size, 3, 224, 224)\n",
        "        \"input_ids\": torch.tensor(\n",
        "            [item[\"input_ids\"] for item in items]\n",
        "        ),  # (batch_size, max_length)\n",
        "        \"attention_mask\": torch.tensor(\n",
        "            [item[\"attention_mask\"] for item in items]\n",
        "        ),  # (batch_size, max_length)\n",
        "        \"label_id\": torch.tensor([item[\"label_id\"] for item in items]),  # (batch_size),\n",
        "        \"return_loss\": True,\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_clip_classifier(\n",
        "    model: nn.Module,\n",
        "    dataset: Dataset,\n",
        "    tokenizer: CLIPTokenizerFast,\n",
        "    labels: list[str],\n",
        "    batch_size: int = 64,\n",
        "    num_workers: int = 5,\n",
        "    device: str = \"cuda\",\n",
        ") -> None:\n",
        "    \"\"\"Function evaluates CLIP model on given dataset\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): CLIP model\n",
        "        dataset (Dataset): evaluation dataset\n",
        "        tokenizer (CLIPTokenizerFast): CLIP tokenizer\n",
        "        labels (list[str]): list with dataset labels\n",
        "        batch_size (int): batch size. Defaults to 64\n",
        "        num_workers (int): number of workers for dataloader. Defaults to 5\n",
        "        device (str, optional): model's device. Defaults to \"cuda\".\n",
        "    \"\"\"\n",
        "    clip_classifier = CLIPClassifier(model, tokenizer, labels)\n",
        "    test_dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "    clip_classifier = clip_classifier.to(device)\n",
        "    acc = calculate_accuracy(clip_classifier, test_dataloader)\n",
        "    print(f\"Model accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5619d9-890b-476f-b274-14f0208a86de",
      "metadata": {
        "id": "0f5619d9-890b-476f-b274-14f0208a86de"
      },
      "source": [
        "### Evaluate baseline CLIP (Zero-shot)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe1788b-7396-4fcd-9493-6d2dc13d1735",
      "metadata": {
        "id": "dbe1788b-7396-4fcd-9493-6d2dc13d1735"
      },
      "source": [
        "First, let's evaluate our pretrained CLIP model on test dataset. One of the advantages of CLIP model compared to standard classification models is that it works well without fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6aa412e-c74a-4bef-a5a5-381ac68c3ac8",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a423fc444cc54d1c9a8d1bf7b69916be"
          ]
        },
        "id": "c6aa412e-c74a-4bef-a5a5-381ac68c3ac8",
        "outputId": "330dcd17-1250-4b5b-ef29-05e892bacd60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 0.0000M || All params: 151.2773M || Trainable%: 0.00%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a423fc444cc54d1c9a8d1bf7b69916be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluate model on dataset:   0%|          | 0/28 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 0.8003384094754653\n"
          ]
        }
      ],
      "source": [
        "clip_baseline = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "freeze_params(clip_baseline)\n",
        "print_trainable_parameters(clip_baseline)\n",
        "evaluate_clip_classifier(clip_baseline, dataset['test'], TOKENIZER, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0662037f-7a6e-4f21-9383-68207cec1a76",
      "metadata": {
        "id": "0662037f-7a6e-4f21-9383-68207cec1a76"
      },
      "source": [
        "In our case we got 0.8 accuracy for free! But we can improve it even further our metrics with fine-tuning techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e31770f-6491-4fd7-a3d7-3eff8b723fb2",
      "metadata": {
        "id": "7e31770f-6491-4fd7-a3d7-3eff8b723fb2"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0ff0e9-45c0-4a95-8a33-59030d5d2d98",
      "metadata": {
        "id": "6a0ff0e9-45c0-4a95-8a33-59030d5d2d98"
      },
      "source": [
        "In this section, we'll undertake the fine-tuning process leveraging the [ğŸ¤—HuggingFace Trainer API](https://huggingface.co/docs/transformers/main_classes/trainer). The Trainer API significantly streamlines the training code, eliminating much of the boilerplate, and offers various features such as the ability to save model weights and training in fp16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46ab3ba-524c-4842-88a7-fda209a813f0",
      "metadata": {
        "id": "b46ab3ba-524c-4842-88a7-fda209a813f0"
      },
      "outputs": [],
      "source": [
        "def collate_train_fn(items: LazyBatch):\n",
        "    items = collate_fn(items)\n",
        "    items.pop(\"label_id\")\n",
        "    return items\n",
        "\n",
        "\n",
        "def get_default_training_args(\n",
        "    experiment_name: str,\n",
        "    lr: float,\n",
        "    batch_size: int = 256,\n",
        "    num_epoch: int = 2,\n",
        "    num_workers: int = 15,\n",
        ") -> TrainingArguments:\n",
        "    \"\"\"Function gets default training arguments for HF Trainer\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): experiment name\n",
        "        lr (float): learning rate\n",
        "        batch_size (int, optional): batch size. Defaults to 256.\n",
        "        num_epoch (int, optional): number of epoch to train. Defaults to 2.\n",
        "        num_workers (int, optional): number of workers for dataloader. Defaults to 15.\n",
        "\n",
        "    Returns:\n",
        "        TrainingArguments: arguments object for HF Trainer\n",
        "    \"\"\"\n",
        "    return TrainingArguments(\n",
        "        experiment_name,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        learning_rate=lr,\n",
        "        num_train_epochs=num_epoch,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=1,\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False,\n",
        "        load_best_model_at_end=True,\n",
        "        dataloader_num_workers=num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1294e881-db65-4cf9-9bfa-3cbb6e80ff01",
      "metadata": {
        "id": "1294e881-db65-4cf9-9bfa-3cbb6e80ff01"
      },
      "source": [
        "### All layers tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07a0df3-4f68-47ad-b889-681410a41b8a",
      "metadata": {
        "id": "a07a0df3-4f68-47ad-b889-681410a41b8a"
      },
      "source": [
        "Initially, let's explore the simplest approach to fine-tuning, which involves fine-tuning all layers of the model. While straightforward, it's important to note that this method is computationally intensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660a05ec-8113-4460-ad67-d2ffd346a6b2",
      "metadata": {
        "id": "660a05ec-8113-4460-ad67-d2ffd346a6b2",
        "outputId": "11c0ca4c-3b7e-4bbb-daa1-521136242f50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 00:35, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.477600</td>\n",
              "      <td>2.269790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.290000</td>\n",
              "      <td>2.232479</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=42, training_loss=2.398661812146505, metrics={'train_runtime': 44.6052, 'train_samples_per_second': 231.946, 'train_steps_per_second': 0.942, 'total_flos': 70339764267612.0, 'train_loss': 2.398661812146505, 'epoch': 2.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clip_full_finetuned = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "trainer = Trainer(\n",
        "    model=clip_full_finetuned,\n",
        "    args=get_default_training_args(\"clip-all-layers-tuning-oxford-pets\", 3e-6),\n",
        "    data_collator=collate_train_fn,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86733e8d-66d0-445a-86f6-d18f728d1982",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2f44dd20a1c246bd9604a45accc9dbf5"
          ]
        },
        "id": "86733e8d-66d0-445a-86f6-d18f728d1982",
        "outputId": "33efd156-aed8-4ace-9061-bb2e24e0c983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 151.2773M || All params: 151.2773M || Trainable%: 100.00%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f44dd20a1c246bd9604a45accc9dbf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluate model on dataset:   0%|          | 0/28 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 0.9278059785673999\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(clip_full_finetuned)\n",
        "evaluate_clip_classifier(clip_full_finetuned, dataset['test'], TOKENIZER, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4893a75d-fae2-419a-9058-99f5b61a0f56",
      "metadata": {
        "id": "4893a75d-fae2-419a-9058-99f5b61a0f56"
      },
      "source": [
        "### Fine-tuning Text Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2c718b-35e8-4498-822a-25f6658a5d50",
      "metadata": {
        "id": "0b2c718b-35e8-4498-822a-25f6658a5d50"
      },
      "source": [
        "Following that, we have the option to fine-tune only the language model while keeping the visual model weights frozen. This method proves beneficial when dataset labels are provided in a language that differs from the one in which the language model was originally trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6534e8c-5b86-4925-ae69-dde621cf5a33",
      "metadata": {
        "id": "f6534e8c-5b86-4925-ae69-dde621cf5a33",
        "outputId": "0ca1db76-7e92-4478-95d8-d735efa10c26"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 00:32, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.477200</td>\n",
              "      <td>2.262403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.331100</td>\n",
              "      <td>2.229800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=42, training_loss=2.395583402542841, metrics={'train_runtime': 40.0923, 'train_samples_per_second': 258.054, 'train_steps_per_second': 1.048, 'total_flos': 70339764267612.0, 'train_loss': 2.395583402542841, 'epoch': 2.0})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clip_text_model_tuning = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "freeze_params(clip_text_model_tuning.vision_model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=clip_text_model_tuning,\n",
        "    args=get_default_training_args(\"clip-text-model-tuning-oxford-pets\", 3e-5),\n",
        "    data_collator=collate_train_fn,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3152ce73-ac4b-4b7d-abe0-e260d12d967d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3c5bb8f49ae6463c97db810d0dc271d7"
          ]
        },
        "id": "3152ce73-ac4b-4b7d-abe0-e260d12d967d",
        "outputId": "72359de0-e6ba-46f7-af3d-f19e2e891d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 63.8213M || All params: 151.2773M || Trainable%: 42.19%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c5bb8f49ae6463c97db810d0dc271d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluate model on dataset:   0%|          | 0/28 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 0.9199097574732092\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(clip_text_model_tuning)\n",
        "evaluate_clip_classifier(clip_text_model_tuning, dataset['test'], TOKENIZER, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b52aaec-780b-462c-9617-6e895495cb34",
      "metadata": {
        "id": "1b52aaec-780b-462c-9617-6e895495cb34"
      },
      "source": [
        "### Fine-tuning Image Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6968264-b3e4-4f8a-bb6d-04265832105a",
      "metadata": {
        "id": "e6968264-b3e4-4f8a-bb6d-04265832105a"
      },
      "source": [
        "We can do the opposite and fine-tune only vision model. This can be useful if the image domain in your dataset is quite specific, such as satellite images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b071345e-4930-4dc3-bf62-ef81bf4c4308",
      "metadata": {
        "id": "b071345e-4930-4dc3-bf62-ef81bf4c4308",
        "outputId": "9037c950-c2d0-427d-ecfa-5a71551700e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 00:34, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.532900</td>\n",
              "      <td>2.441825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.245000</td>\n",
              "      <td>2.330577</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=42, training_loss=2.4347278277079263, metrics={'train_runtime': 42.5156, 'train_samples_per_second': 243.346, 'train_steps_per_second': 0.988, 'total_flos': 70339764267612.0, 'train_loss': 2.4347278277079263, 'epoch': 2.0})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clip_vision_model_tuning = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "freeze_params(clip_vision_model_tuning.text_model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=clip_vision_model_tuning,\n",
        "    args=get_default_training_args(\"clip-vision-model-tuning-oxford-pets\", 3e-5),\n",
        "    data_collator=collate_train_fn,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7662e00b-7819-4aa6-84fd-03dbc20bbdf5",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "dcd185ede6b84d39855c7b03040b102f"
          ]
        },
        "id": "7662e00b-7819-4aa6-84fd-03dbc20bbdf5",
        "outputId": "0f37c607-0250-41fe-e9aa-5d7d9db274f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 88.1114M || All params: 151.2773M || Trainable%: 58.24%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcd185ede6b84d39855c7b03040b102f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluate model on dataset:   0%|          | 0/28 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 0.924985899605189\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(clip_vision_model_tuning)\n",
        "evaluate_clip_classifier(clip_vision_model_tuning, dataset['test'], TOKENIZER, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7600733-cf2a-424f-8f34-8b2e925f9656",
      "metadata": {
        "id": "d7600733-cf2a-424f-8f34-8b2e925f9656"
      },
      "source": [
        "### Fine-tune 30 percent of the Last Layers of Vision-Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ee550c1-4fb8-432f-a9f8-0a7cbc96d68b",
      "metadata": {
        "id": "9ee550c1-4fb8-432f-a9f8-0a7cbc96d68b"
      },
      "source": [
        "The most universal option is to fine-tune only the last layers in both models. This approach strikes a balance, allowing adaptation to the specific task while leveraging the pre-trained knowledge from the lower layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac157de-c028-46aa-91cf-29c92ed903a1",
      "metadata": {
        "id": "cac157de-c028-46aa-91cf-29c92ed903a1"
      },
      "outputs": [],
      "source": [
        "clip_partial_tuning = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "freeze_params(clip_partial_tuning.text_model, freeze_top_percent=0.7)\n",
        "freeze_params(clip_partial_tuning.vision_model, freeze_top_percent=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05c7453-c488-4fa1-b8c7-66b4aeaa6fc9",
      "metadata": {
        "id": "a05c7453-c488-4fa1-b8c7-66b4aeaa6fc9",
        "outputId": "f6ea652b-a210-491f-9b99-220468db5824"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 00:32, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.389500</td>\n",
              "      <td>2.181050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.209200</td>\n",
              "      <td>2.150931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=42, training_loss=2.3123262382688976, metrics={'train_runtime': 40.1759, 'train_samples_per_second': 257.517, 'train_steps_per_second': 1.045, 'total_flos': 70339764267612.0, 'train_loss': 2.3123262382688976, 'epoch': 2.0})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=clip_partial_tuning,\n",
        "    args=get_default_training_args(\"clip-partial-model-tuning-oxford-pets\", 3e-5),\n",
        "    data_collator=collate_train_fn,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84cc5eec-80e7-42ee-b3dd-34b05c9a0072",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5cc6c1736d4c447c80595d345708c9dc"
          ]
        },
        "id": "84cc5eec-80e7-42ee-b3dd-34b05c9a0072",
        "outputId": "8bfd8bab-2b60-44ee-cc68-82cfac6c171c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 38.7971M || All params: 151.2773M || Trainable%: 25.65%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cc6c1736d4c447c80595d345708c9dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluate model on dataset:   0%|          | 0/28 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 0.9334461364918217\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(clip_partial_tuning)\n",
        "evaluate_clip_classifier(clip_partial_tuning, dataset['test'], TOKENIZER, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a57519-8f58-4b92-b4a0-8c6edb36e189",
      "metadata": {
        "id": "f9a57519-8f58-4b92-b4a0-8c6edb36e189"
      },
      "source": [
        "### PEFT tuning (with LoRA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e2c4de-adcc-41a7-ad17-9fb3a30a2bfd",
      "metadata": {
        "id": "61e2c4de-adcc-41a7-ad17-9fb3a30a2bfd"
      },
      "source": [
        "In cases where you want to fine-tune a pre-trained model with a large number of parameters, you can use the [PEFT](https://huggingface.co/blog/peft)(Parameter-Efficient Fine-Tuning) techniques. PEFT allows you to train a small number of parameters (about 1-5% of all model parameters) with performance comparable to training all layers\n",
        "\n",
        "In our case, we will use the [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora) technique using the [ğŸ¤—HuggingFace PEFT](https://huggingface.co/docs/peft/index) library.\n",
        "\n",
        "**LoRA** introduces two small-rank matrices (called *update matrices*) to the initial weights of the pretrained model. Throughout the fine-tuning process, the initial weights of the model remain frozen, and only the update matrices are trained.\n",
        "\n",
        "In our specific case, we'll employ update matrices of rank 64, applied to the query, key, and value matrices in both the visual and language components of the CLIP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074d8f7c-f00c-4783-ad09-eb913a385af7",
      "metadata": {
        "id": "074d8f7c-f00c-4783-ad09-eb913a385af7"
      },
      "outputs": [],
      "source": [
        "clip_lora_tuning = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=64,\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj'],\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(clip_lora_tuning, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9201e73-e832-4f7a-bcaa-e23c04b4bc1e",
      "metadata": {
        "id": "f9201e73-e832-4f7a-bcaa-e23c04b4bc1e",
        "outputId": "352e33e0-1cc6-4478-f819-141ed0839f35"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 00:31, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.410300</td>\n",
              "      <td>2.221336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.251800</td>\n",
              "      <td>2.195205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=42, training_loss=2.320029849097842, metrics={'train_runtime': 40.0491, 'train_samples_per_second': 258.333, 'train_steps_per_second': 1.049, 'total_flos': 73635016583772.0, 'train_loss': 2.320029849097842, 'epoch': 2.0})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=get_default_training_args(\"clip-lora-model-tuning-oxford-pets\", 3e-4),\n",
        "    data_collator=collate_train_fn,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc52eef-2901-4d6a-96d1-787ae4a29c09",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7d524ca329cb4252a7e4664cf43ba4f2"
          ]
        },
        "id": "dfc52eef-2901-4d6a-96d1-787ae4a29c09",
        "outputId": "b8dd4ff4-4763-4e2f-ac28-d2c45c899367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 5.8982M || All params: 157.1756M || Trainable%: 3.75%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d524ca329cb4252a7e4664cf43ba4f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluate model on dataset:   0%|          | 0/28 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 0.9261139311900733\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(lora_model)\n",
        "evaluate_clip_classifier(lora_model, dataset['test'], TOKENIZER, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "077a1adc-0765-4b83-b4e2-971a4734e2e6",
      "metadata": {
        "id": "077a1adc-0765-4b83-b4e2-971a4734e2e6"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7fef04e-b1f5-475d-a4fb-77b1114bec75",
      "metadata": {
        "id": "b7fef04e-b1f5-475d-a4fb-77b1114bec75"
      },
      "source": [
        "In this notebook, we explored the fine-tuning of a CLIP model on the Oxford-pets dataset, assessing the model's performance in zero-shot mode and through various fine-tuning methods, including PEFT.\n",
        "\n",
        "The achieved results are summarized below:\n",
        "\n",
        "| Model                       | Trainable Parameters (M) |  Trainable Parameters (%) | Accuracy    |\n",
        "| --------------------------- | ------------------------ | ------------------------- | ----------- |\n",
        "| Baseline (Zero-shot)        | 0                        | 0%                        | 0.8003      |\n",
        "| Full fine-tuning            | 151.28                   | 100%                      | 0.9278      |\n",
        "| Only language model tuning  | 63.82                    | 42.19%                    | 0.9199      |\n",
        "| Only vision model tuning    | 88.11                    | 58.24%                    | 0.9250      |\n",
        "|Top 30% of both models tuning| 38.79                    | 25.65%                    | **0.9334**  |  \n",
        "| LoRA tuning                 | **5.90**                 | **3.75%**                 | 0.9261      |\n",
        "\n",
        "In our case, the best result was obtained from the model with fine-tuning only top 30% of layers.\n",
        "\n",
        "Feel free to experiment with training parameters in the Trainer and CLIP model settings to potentially enhance the results further!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}